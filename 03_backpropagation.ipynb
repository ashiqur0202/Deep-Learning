{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_prime(Z):\n",
    "    return np.where(Z > 0, 1, 0)\n",
    "\n",
    "def cost(yHat, y):\n",
    "    return 0.5 * np.sum((yHat - y)**2)\n",
    "\n",
    "def cost_prime(yHat, y):\n",
    "    return yHat - y\n",
    "\n",
    "def backprop(x, y, Wh, Wo, lr):\n",
    "    # Forward propagation\n",
    "    Zh = np.dot(x, Wh)\n",
    "    H = relu_prime(Zh)\n",
    "\n",
    "    Zo = np.dot(H, Wo)\n",
    "    yHat = relu_prime(Zo)\n",
    "\n",
    "    # Layer Error\n",
    "    Eo = cost_prime(yHat, y) * relu_prime(Zo)\n",
    "    Eh = np.dot(Eo, Wo.T) * relu_prime(Zh)\n",
    "\n",
    "    # Cost derivative for weights\n",
    "    dWo = np.dot(H.T, Eo)\n",
    "    dWh = np.dot(x.T, Eh)\n",
    "\n",
    "    # Update weights\n",
    "    Wh -= lr * dWh\n",
    "    Wo -= lr * dWo\n",
    "\n",
    "# Example usage\n",
    "# Assuming x, y, Wh, Wo, lr are defined elsewhere\n",
    "x = np.random.rand(2, 3)  # Example input matrix with shape (2, 3)\n",
    "y = np.random.rand(2, 3)  # Example target matrix with shape (2, 3)\n",
    "Wh = np.random.rand(3, 3)  # Example weights matrix for hidden layer\n",
    "Wo = np.random.rand(3, 3)  # Example weights matrix for output layer\n",
    "lr = 0.01  # Example learning rate\n",
    "\n",
    "backprop(x, y, Wh, Wo, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.91849798, 0.31459573, 0.15589525],\n",
       "        [0.50063489, 0.81648542, 0.73793416]]),\n",
       " array([[0.56485251, 0.68927135, 0.55176539],\n",
       "        [0.06018269, 0.01881898, 0.68293117]]),\n",
       " array([[0.61179315, 0.18823299, 0.81642904],\n",
       "        [0.21938286, 0.34381293, 0.48395619],\n",
       "        [0.73458135, 0.19232727, 0.16410045]]),\n",
       " array([[0.43221483, 0.46838498, 0.47532897],\n",
       "        [0.7697312 , 0.82596855, 0.41345834],\n",
       "        [0.16945669, 0.35066125, 0.51430952]]),\n",
       " 0.01)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, Wh, Wo, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_16340\\805098393.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_16340\\805098393.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_16340\\805098393.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Wh = torch.tensor(Wh, dtype=torch.float32, requires_grad=True)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_16340\\805098393.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Wo = torch.tensor(Wo, dtype=torch.float32, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def relu_prime(Z):\n",
    "    return torch.where(Z > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "\n",
    "def cost(yHat, y):\n",
    "    return 0.5 * torch.sum((yHat - y)**2)\n",
    "\n",
    "def cost_prime(yHat, y):\n",
    "    return yHat - y\n",
    "\n",
    "def backprop(x, y, Wh, Wo, lr):\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    Wh = torch.tensor(Wh, dtype=torch.float32, requires_grad=True)\n",
    "    Wo = torch.tensor(Wo, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    # Forward propagation\n",
    "    Zh = torch.matmul(x, Wh)\n",
    "    H = relu_prime(Zh)\n",
    "\n",
    "    Zo = torch.matmul(H, Wo)\n",
    "    yHat = relu_prime(Zo)\n",
    "\n",
    "    # Layer Error\n",
    "    Eo = cost_prime(yHat, y) * relu_prime(Zo)\n",
    "    Eh = torch.matmul(Eo, Wo.t()) * relu_prime(Zh)\n",
    "\n",
    "    # Cost derivative for weights\n",
    "    dWo = torch.matmul(H.t(), Eo)\n",
    "    dWh = torch.matmul(x.t(), Eh)\n",
    "\n",
    "    # Update weights\n",
    "    with torch.no_grad():\n",
    "        Wh -= lr * dWh\n",
    "        Wo -= lr * dWo\n",
    "\n",
    "# Example usage\n",
    "# Assuming x, y, Wh, Wo, lr are defined elsewhere\n",
    "x = torch.rand((2, 3), dtype=torch.float32)\n",
    "y = torch.rand((2, 3), dtype=torch.float32)\n",
    "Wh = torch.rand((3, 3), dtype=torch.float32, requires_grad=True)\n",
    "Wo = torch.rand((3, 3), dtype=torch.float32, requires_grad=True)\n",
    "lr = 0.01\n",
    "\n",
    "backprop(x, y, Wh, Wo, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6442, 0.9653, 0.4251],\n",
       "         [0.6686, 0.0571, 0.2971]]),\n",
       " tensor([[0.2838, 0.3286, 0.2702],\n",
       "         [0.1200, 0.4288, 0.6314]]),\n",
       " tensor([[0.3451, 0.5670, 0.5865],\n",
       "         [0.7618, 0.7717, 0.2840],\n",
       "         [0.4032, 0.7573, 0.0625]], requires_grad=True),\n",
       " tensor([[0.1922, 0.0263, 0.9965],\n",
       "         [0.5663, 0.1059, 0.9733],\n",
       "         [0.4465, 0.0786, 0.8636]], requires_grad=True),\n",
       " 0.01)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, Wh, Wo, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create a random 2x3 tensor and set requires_grad=True to track computation\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "print(\"Input Tensor:\")\n",
    "print(x)\n",
    "\n",
    "# Define a simple linear layer with 3 input features and 2 output features\n",
    "linear_layer = nn.Linear(3, 2)\n",
    "\n",
    "# Forward pass\n",
    "y = linear_layer(x)\n",
    "print(\"\\nOutput Tensor after Forward Pass:\")\n",
    "print(y)\n",
    "\n",
    "# Create a random target tensor\n",
    "target = torch.randn(2, 2)\n",
    "\n",
    "# Define a loss function, here we use Mean Squared Error (MSE) loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(y, target)\n",
    "print(\"\\nLoss after Forward Pass:\")\n",
    "print(loss.item())\n",
    "\n",
    "# Perform backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# Access the gradients of the input tensor (dx/dw)\n",
    "gradients = x.grad\n",
    "print(\"\\nGradients after Backward Pass:\")\n",
    "print(gradients)\n",
    "\n",
    "# Update the weights using an optimizer (e.g., Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(linear_layer.parameters(), lr=0.01)\n",
    "optimizer.step()\n",
    "\n",
    "# Check if the weights have been updated\n",
    "updated_weights = linear_layer.weight\n",
    "print(\"\\nUpdated Weights after Optimization:\")\n",
    "print(updated_weights)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
